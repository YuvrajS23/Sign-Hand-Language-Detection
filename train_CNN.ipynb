{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc8610cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Flatten, BatchNormalization, Conv2D, MaxPool2D, Dropout\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import itertools\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import cv2\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bc3fff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'train/'\n",
    "test_path = 'test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "658b1fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2110 images belonging to 10 classes.\n",
      "Found 900 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "train_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input).flow_from_directory(directory=train_path, target_size=(64,64), class_mode='categorical', batch_size=10,shuffle=True)\n",
    "test_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input).flow_from_directory(directory=test_path, target_size=(64,64), class_mode='categorical', batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "489ebcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, labels = next(train_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1454e63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2160x1440 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 64, 64, 3)\n",
      "[[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "def plotImages(images_arr):\n",
    "    fig, axes = plt.subplots(1, 10, figsize=(30,20))\n",
    "    axes = axes.flatten()\n",
    "    for img, ax in zip( images_arr, axes):\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plotImages(imgs)\n",
    "print(imgs.shape)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "991edbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(64,64,3)))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding = 'same'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding = 'valid'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(64,activation =\"relu\"))\n",
    "model.add(Dense(128,activation =\"relu\"))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(128,activation =\"relu\"))\n",
    "#model.add(Dropout(0.3))\n",
    "model.add(Dense(10,activation =\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fcfa1005",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.0001)\n",
    "# early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "\n",
    "# model.compile(optimizer=SGD(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.0005)\n",
    "# early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30c70383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "211/211 [==============================] - 7s 31ms/step - loss: 1.3704 - accuracy: 0.7009 - val_loss: 0.0664 - val_accuracy: 0.9867\n",
      "Epoch 2/10\n",
      "211/211 [==============================] - 6s 29ms/step - loss: 0.0587 - accuracy: 0.9839 - val_loss: 0.0727 - val_accuracy: 0.9811\n",
      "Epoch 3/10\n",
      "211/211 [==============================] - 6s 29ms/step - loss: 0.0596 - accuracy: 0.9858 - val_loss: 0.3304 - val_accuracy: 0.9256\n",
      "Epoch 4/10\n",
      "211/211 [==============================] - 6s 28ms/step - loss: 0.1475 - accuracy: 0.9678 - val_loss: 0.1044 - val_accuracy: 0.9733\n",
      "Epoch 5/10\n",
      "211/211 [==============================] - 6s 30ms/step - loss: 0.0841 - accuracy: 0.9782 - val_loss: 0.2863 - val_accuracy: 0.9322\n",
      "Epoch 6/10\n",
      "211/211 [==============================] - 7s 31ms/step - loss: 0.1165 - accuracy: 0.9754 - val_loss: 0.1819 - val_accuracy: 0.9600\n",
      "Epoch 7/10\n",
      "211/211 [==============================] - 7s 31ms/step - loss: 0.0096 - accuracy: 0.9967 - val_loss: 0.0740 - val_accuracy: 0.9878\n",
      "Epoch 8/10\n",
      "211/211 [==============================] - 7s 31ms/step - loss: 0.0905 - accuracy: 0.9820 - val_loss: 0.2427 - val_accuracy: 0.9544\n",
      "Epoch 9/10\n",
      "211/211 [==============================] - 7s 32ms/step - loss: 0.0617 - accuracy: 0.9900 - val_loss: 0.0326 - val_accuracy: 0.9922\n",
      "Epoch 10/10\n",
      "211/211 [==============================] - 6s 30ms/step - loss: 0.0058 - accuracy: 0.9986 - val_loss: 0.0595 - val_accuracy: 0.9944\n",
      "loss of 0.44547685980796814; accuracy of 89.99999761581421%\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_batches, epochs=10, validation_data = test_batches)#, checkpoint])\n",
    "imgs, labels = next(train_batches) # For getting next batch of imgs...\n",
    "scores = model.evaluate(imgs, labels, verbose=0)\n",
    "print(f'{model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4dddf99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('89_percent.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d8fdd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"89_percent.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42a448b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss of 0.44547685980796814; accuracy of 89.99999761581421%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(imgs, labels, verbose=0)\n",
    "print(f'{model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66e4354f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 62, 62, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 31, 31, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 31, 31, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPoolin  (None, 15, 15, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 13, 13, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPoolin  (None, 6, 6, 128)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 4608)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                294976    \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 414346 (1.58 MB)\n",
      "Trainable params: 414346 (1.58 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10c1c74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, labels = next(test_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c83e5b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = {0:'Zero',1:'One',2:'Two',3:'Three',4:'Four',5:'Five',6:'Six',7:'Seven',8:'Eight',9:'Nine'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "43915443",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(imgs, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c1bfc2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions on a small set of test data--\n",
      "\n",
      "Three   Nine   Three   Five   Two   One   Eight   Two   Six   Zero   "
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAACGgAAADaCAYAAADw3eaaAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAQ5UlEQVR4nO3d25KjxhIF0NEJ/f8v93lwOJrBQkLApi651qvH07REVhVMxs7Hz8/PHwAAAAAAAAAAcv7X+gIAAAAAAAAAAGanQQMAAAAAAAAAIEyDBgAAAAAAAABAmAYNAAAAAAAAAIAwDRoAAAAAAAAAAGEaNAAAAAAAAAAAwp7v/uPj8fi560JgBj8/P489f05twXfUFmSoLchQW5ChtiBDbUGG2oIMtQUZe2pLXcF3tupKggYAAAAAAAAAQJgGDQAAAAAAAACAMA0aAAAAAAAAAABhGjQAAAAAAAAAAMI0aAAAAAAAAAAAhGnQAAAAAAAAAAAI06ABAAAAAAAAABCmQQMAAAAAAAAAIEyDBgAAAAAAAABAmAYNAAAAAAAAAIAwDRoAAAAAAAAAAGEaNAAAAAAAAAAAwjRoAAAAAAAAAACEadAAAAAAAAAAAAh7tr4AAGAOPz8/X/8/j8cjcCUAAAAAAAD9kaABAAAAAAAAABCmQQMAAAAAAAAAIMyIEwCYxHLEyF2jQ46MNQEAAAAAAN6/YzcifE4SNAAAAAAAAAAAwjRoAAAAAAAAAACEGXECAIPZM1akxbgTAAAAAADgvb2jw8+OGPdvA32SoAEAAAAAAAAAEKZBAwAAAAAAAAAgTIMGAAAAAAAAAEDYs/UFAAAAAHDc1lxi84YBAACytp7HenDk2jxH5knQAAAAAAAAAAAI06ABAAAAAAAAABBmxAkATG4dYyaiDABgfHuiapd/xhkQAADgv/aOAVk+U/U81uQsz5F5EjQAAAAAAAAAAMI0aAAAAAAAAAAAhBlxAgAAANC5mSN0AWBUR2LxAWjvyPOVZzKuIkEDAAAAAAAAACBMgwYAAAAAAAAAQJgRJwAAAAAAADuIuAegiuWeZ1zXdSRoAAAAAAAAAACEadAAAAAAAAAAAAgz4gQAAAAAAGhq7+iQFhHrxpoAUJ1xJ9eRoAEAAAAAAAAAEKZBAwAAAAAAAAAgTIMGAAAAAAAAAEDYs/UFAABjMXcVAAAAuMKRdwzL/+fxeFx5OQBMzHtteiFBAwAAAAAAAAAgTIMGAAAAAAAAAECYEScAUIwoUAA4ZisO1X4KQGX2RwCgV8aaZKw/V+e+70jQAAAAAAAAAAAI06ABAAAAAAAAABBmxAkAAAxibyxjKlaw9c+HO4g/pVfrtdW9CrSyd/0xXhOgndT4qXd7gLWeXtzxrJSsJeYnQQMAAAAAAAAAIEyDBgAAAAAAAABAmBEnAAAwmSvjpEUukmJkDnzHegy0ZA0C6N+etXr9Z/Y8b9kD6N2I9+i72hvx9zHa7jsSNAAAAAAAAAAAwjRoAAAAAAAAAACEGXHCLuKHAQDaGDHWELYcuZ/FZAJQ1da+aT+EMahVqvHsBrCPBA0AAAAAAAAAgDANGgAAAAAAAAAAYRo0AAAAAAAAAADCnq0vgG1H5jNfOdfLvHOqeXfPm5nHrPbMhrQfAACMb32m84wDfdrz/LXnOQ44x7sQGMOeWrVXAr2RoAEAAAAAAAAAEKZBAwAAAAAAAAAgzIgToJwjEYXiQwEAgLuJVweA97yzg3NGrCFnZGB0EjQAAAAAAAAAAMI0aAAAAAAAAAAAhBlx0hnRTDC/vXU+SqQc8xgx0hAqWNZjlbPi1u9pbRpfxfsZAN6xH0I/1CNcx3tG7rS+x+5Yz93jnCFBAwAAAAAAAAAgTIMGAAAAAAAAAECYEScANxCRCMBI7Fv0SoQoAABbnBUho/WoSu8oYCz2488kaAAAAAAAAAAAhGnQAAAAAAAAAAAIM+KkM62jogAA4C7Ou/TCvQgAAPCa5yWAa0nQAAAAAAAAAAAI06ABAAAAAAAAABCmQQMAAAAAAAAAIOzZ+gIAeG052+/xeDS8EipqMVvyyM9UG/DZ6PvJiNcMMIIrz3vWajhuXT8tnsUAmMNyTzmyn2y9P0j9vXvZK/nW2Xv2W+uf4fmITyRoAAAAAAAAAACEadAAAAAAAAAAAAgz4mQyZ6OiRENRwd3xVnC1me7hs9c/+ugGqGb0NYu+ePYBgHF4duNue+PmnQnhsxHrxF5DSyPWDPeSoAEAAAAAAAAAEKZBAwAAAAAAAAAgzIgToBwjFeo5+537nsegNpnVXbGI4hcBOMo5DI678h3Fnz81a3DrM6z4WQBcJfWO4Mi+5X0FMBsJGgAAAAAAAAAAYRo0AAAAAAAAAADCjDjpTIuoJvFQAO+9WydFpgK856zJHYxXgH6oR+jf3nh55zgAYHTLc46zDb2QoAEAAAAAAAAAEKZBAwAAAAAAAAAgzIgTAKa3jmu9MspMhDMAjEOcKQBVvdsDR9sfR7te+tPrmHHvleCXtR6YmQQNAAAAAAAAAIAwDRoAAAAAAAAAAGEaNAAAAAAAAAAAwp6tL4Cc5Ywu8+vgfiPOyasyD3P5O1z5PVl3+7H+Xn0f8I8R9yb4lj0AvrdVJ2f3DedjuM6RGnL2YwSpdzRrW3vSXT/z7J+3jwLAPCRoAAAAAAAAAACEadAAAAAAAAAAAAgz4qQoEYfAnz/WgiRxzjA/8bPbRtlfqoz2oi33GfzDvgnnjHK+Stn7+1tPAMZVfa8j764RV9U5j30mQQMAAAAAAAAAIEyDBgAAAAAAAABAmBEnAMD0jJzhKnvjD91zfRJfyV1xpt/+3es/b92orcVa1Xp9tG9CLe/2vdbrETWMeJ/ZK5nViPUIcJYEDQAAAAAAAACAMA0aAAAAAAAAAABhRpwAFCM27m93RKkmY8tFwX5PLCh3c8+1dWRt9D3Ny15Jr9ybv+ybUI81EABgbJ7dviNBAwAAAAAAAAAgTIMGAAAAAAAAAECYESdFiApkJnvvZ5FK1/FZXisV27z1d9kDgNGcXRute2NIjela3z933A9X/wwjHmqwVkFd6h8AoC2jw2lJggYAAAAAAAAAQJgGDQAAAAAAAACAMA0aAAAAAAAAAABhz9YXkHR2ZpBZv+cc+fx95uxx5D5JzRAbZTbZKNfZWou5c6kZ877zfVKfP1zJTMxtV34e1gCA/qzXeWs18Iq1YR6efQCACiRoAAAAAAAAAACEadAAAAAAAAAAAAibesTJWWLPv3c2es5nzpWqRyEa83TOiONOqt/zMAJnnb74DuYxcxy2dQN+qQcA1mY+B26xH8Jr6gEYhQQNAAAAAAAAAIAwDRoAAAAAAAAAAGFTjzi5Mt7srtiwipFssGWrBrZqUM34DGbybt/xPQOVWPPYw30C9Yh3ZzT2KriOd+i/7IdsqVgn6gEYhQQNAAAAAAAAAIAwDRoAAAAAAAAAAGFTjzhZSo07Wf/dQF6VSLY9rv4srGevtR4x4p4Hqkmte/a5flWM3z1CZO/Y3Ns5agNgPu/exdhTX/PvFvDL+ZBvtH7/PyJ1dY4EDQAAAAAAAACAMA0aAAAAAAAAAABhGjQAAAAAAAAAAMKerS8AqM0sr+/5zNpbzlfzfQBHHZkP/O7PWI9eMxOTWZmpPAZr8/3UBsAc7KHAVY68fwFIkqABAAAAAAAAABCmQQMAAAAAAAAAIMyIkwuIz8zwuc5LROH3fGYAsJ+zIwDQA8/yQEver8NragNoTYIGAAAAAAAAAECYBg0AAAAAAAAAgLCSI07WkUVXxg2KRspYf0c+W7iGWjpv+RmKrwXSrDPb7GlAL6zV/fAuAWAs9lAAeuCdP2kSNAAAAAAAAAAAwjRoAAAAAAAAAACElRxxcpez4062/h9xOgB9En02HjHXfCtZ59aN76lhAAAAAMjzHu46EjQAAAAAAAAAAMI0aAAAAAAAAAAAhBlx8ueeSPqz405EXv/t7OcJlakZAIC5rJ8XnfcAAAAA+iRBAwAAAAAAAAAgTIMGAAAAAAAAAECYBg0AAAAAAAAAgLBn6wvozXJW73qO71X2zgdO/XwA8u7YTwAAAOiL5z8AgHl4z0+CBA0AAAAAAAAAgDANGgAAAAAAAAAAYUacdEAkDgDAeNZj6pzp8rZGAwL0RAQuQJYzIQAAI5OgAQAAAAAAAAAQpkEDAAAAAAAAACDMiJM3xJICsxD/2da7z9/+AgCc5awH0A/vEwEAgHckaAAAAAAAAAAAhGnQAAAAAAAAAAAIM+KEIYnwBeAoewgAAAAwO+8/AK5llB1XkaABAAAAAAAAABCmQQMAAAAAAAAAIMyIk53E1gCQYH+BeajnDLG8wMjsDW3ZQwAAWHNGhGOWz7Tq6BwJGgAAAAAAAAAAYRo0AAAAAAAAAADCNGgAAAAAAAAAAIQ9W18AAPAPM8pzzMTjbuoZAAAAAIA1CRoAAAAAAAAAAGEaNAAAAAAAAAAAwow4OWAdky62GuiRkQ5jMx4B5qGeAQAAPvPsBMAo7FmcIUEDAAAAAAAAACBMgwYAAAAAAAAAQJgRJxcQYwNAktFaQDXGdMFn6gQAgCXnQwDusv43CnvQdyRoAAAAAAAAAACEadAAAAAAAAAAAAgz4uRixp0AkGavgXGpX4C6jK27h2hdeuLsBwAwP896f//Onsk+k6ABAAAAAAAAABCmQQMAAAAAAAAAIMyIE4YgDgfgNZG5MC7xh/Ca2gAAAABG5Z09n0jQAAAAAAAAAAAI06ABAAAAAAAAABCmQQMAAAAAAAAAIOzZ+gJmtp6d/C/zhqjO/C3IUFvblp/H1v4MrVWvYbUJn6mTuVRf96+kNmB+6hxeUxvwmTqBe3kX/5kEDQAAAAAAAACAMA0aAAAAAAAAAABhRpw0sI5zEWX6mtibGsT6QobagrGpYQCAuTjfwXe8QwdgBtXPgMadvCZBAwAAAAAAAAAgTIMGAAAAAAAAAECYEScAExERBcBI7FvsUT0OlBrc5wAAAFCDBA0AAAAAAAAAgDANGgAAAAAAAAAAYUacdECU6S8x17Wtv//q9QBXUVt/W/7+9h1G4KwIAEBFntcAOMoeQo+84+NfEjQAAAAAAAAAAMI0aAAAAAAAAAAAhBlx0hnxNgCk2WsAGJU9jArc5zA/dQ7fUzcAzKTivmb0+C8JGgAAAAAAAAAAYRo0AAAAAAAAAADCNGgAAAAAAAAAAIQ9W18A2yrOH4IlNQAAAAAAAMd5zw7QFwkaAAAAAAAAAABhGjQAAAAAAAAAAMKMOAGGIIYNMqrX1vJ3Xn4W0KvqNQtLFevBvlVPxfscqlmv52odPrM/AjCTiufB9e9Y7R2HBA0AAAAAAAAAgDANGgAAAAAAAAAAYUacDKJKbJvIXgBasQfB/dQdAABbnBUBAGqodu6ToAEAAAAAAAAAEKZBAwAAAAAAAAAgzIgTYDhVRv7A3dTWr/XvXyFWDWBk9jAqWJ9H3Ou/qsXhAvCr+jnQHsi3nCkB2pOgAQAAAAAAAAAQpkEDAAAAAAAAACDMiJMBVYmgEs/GHtVjDNfUDVdRW8Dd9uxhe9cjeyAVOPcBzMmzGAAAlVV43yFBAwAAAAAAAAAgTIMGAAAAAAAAAECYBg0AAAAAAAAAgLBn6wuAPdYzN2edOcQ56/vCrFa4htqqMfeO8ajNbWoWqGC5vtkDAMDeCMAc7Ge/Zn3HJ0EDAAAAAAAAACBMgwYAAAAAAAAAQJgRJxMQdQO8Mmv0E7SmtqCdd2Ndts7EarYe43+gNus+M/HO75fahs+MCeeIWfca+wbMZ6a6lqABAAAAAAAAABCmQQMAAAAAAAAAIOwxU2QRAAAAAAAAAECPJGgAAAAAAAAAAIRp0AAAAAAAAAAACNOgAQAAAAAAAAAQpkEDAAAAAAAAACBMgwYAAAAAAAAAQJgGDQAAAAAAAACAsP8DfHF7j3aWeIUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 2160x1440 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual labels\n",
      "Three   Nine   Three   Five   Two   One   Eight   Two   Six   Zero   (10, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(imgs, verbose=0)\n",
    "print(\"predictions on a small set of test data--\")\n",
    "print(\"\")\n",
    "for ind, i in enumerate(predictions):\n",
    "    print(word_dict[np.argmax(i)], end='   ')\n",
    "\n",
    "plotImages(imgs)\n",
    "print('Actual labels')\n",
    "for i in labels:\n",
    "    print(word_dict[np.argmax(i)], end='   ')\n",
    "\n",
    "print(imgs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02c53eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
